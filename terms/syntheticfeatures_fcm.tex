\chapter{Synthetic Features}
\label{ch:synthetic features}\index{synthetic-features}

\section{The problem}
When employing a machine learning algorithm, it is typically fed with some form of input which are essentially your features on which the algorithm should learn and build a model. Features are essentially the dataset you have as part of training.

Whilst in concept the above seems straightforward, it often transpires that the various data-points provided in the training dataset do not fit a structure that is easily understood by the algorithm. For this reason, an important pre-processing step is needed to:

\begin{enumerate}
    \item Understand the original data well
    \item Subsequently, if and where needed, generate synthetic features
\end{enumerate}

If we look at the following example \citep{AlbertoTubeSpam}: It contains a number of records used to train a spam / ham classifier for comments on a YouTube video.

\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \resizebox{\textwidth}{!}{\begin{tabular}{lllllll}
      \toprule
                        & \textit{Video} & \textit{Comment ID} & \textit{Author} & \textit{Date} & \textit{Content} & \textit{Class} \\
      \midrule
      \textit & Psy & LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8 & Evgeny Murashkin & 2013-11-08T17:34:21 & just for test I have to say murdev.com & Spam  \\
      \textit & Psy & z13bgdvyluihfv11i22rgxwhuvabzz1os04 & Zielimeek21 & 2013-11-28T21:49:00 & I'm only checking the views & Ham  \\
      \textit & Psy & z13kxpqqssa0hlryd04cc1dxeyyngljjngk & Tasha Lucius & 2014-01-19T13:25:56 & 2 billion....Coming soon & Ham  \\
      \textit & Psy & z12lg1vizrmsgxm3q23oij4aqrjxjdd1p & Holly & 2014-11-06T13:41:30 & Follow me on Twitter @mscalifornia95 & Spam  \\
      \bottomrule
    \end{tabular}}
    \caption{Sample of four rows from the Psy dataset from the YouTube comment training dataset.}
    \label{tab:sf_origdatasample}
\end{table}
\vspace{2mm}

Describing each feature in the original raw dataset:

\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \resizebox{\textwidth}{!}{\begin{tabular}{lll}
      \toprule
                        & \textit{Feature} & \textit{Description} \\
      \midrule
      \textit & \makecell[tl]{Video} & \makecell[tl]{The video this comment was written for. The relevance depends whether the \\ classification model is being built generically for all videos, or a per-video \\ specific model is also considered.} \\
      \textit & \makecell[tl]{Comment ID} & \makecell[tl]{Random comment ID generated by the YouTube comment board system. \\ This probably has no impact on the final class.} \\
      \textit & \makecell[tl]{Author} & \makecell[tl]{The author / account that generated the comment. This has relevance only if \\ this account has a lot of spam comments. \\ If that is the case, two things should happen, none of which are directly \\ related to the machine learning algorithm: \\ \quad - Maintain a blacklist of accounts that are probable spam (if a particular \\ \quad \enspace author often has flagged comments). \\ \quad - Block such accounts.} \\
      \textit & \makecell[tl]{Date} & \makecell[tl]{The date does not directly have a huge relevance on the classification of a \\ comment.} \\
      \textit & \makecell[tl]{Content} & \makecell[tl]{The comment body definitely has a big relevance in the classification result, \\ however, can the whole sentence be easily understood by the algorithm as \\ it is?} \\
      \bottomrule
    \end{tabular}}
    \label{tab:sf_origdataexplained}
\end{table}
\vspace{2mm}

As one can see, there is very little input the machine learning algorithm can reliably take just from using the four features described above. One could easily realise this by asking oneself the following question (in plain English):

\qquad \emph{How can I describe the components of a comment well enough to decide whether it is probably spam or ham?}

One can therefore summarise this problem as: \emph{Having enough data to solve the problem, but very little meta-data to actually understand it and solve it.}

\section{Ways of solving the problem}
A way of solving this problem is to apply a synthetic features approach, sometimes referred to as feature engineering or feature extraction. This is essentially the generation of features derived from other existing features, in a way that can be more easily captured or understood by a machine learning algorithm. It is a way of generating meta-data for the existing features in the original dataset.

There is not a one-size-fits-all approach but rather it is more of an iterative approach with new synthetic features being outputted per iteration, following which one then assesses whether it is enough to generate a reliable machine learning model from the new features or not.

\subsection{Analysing each feature} \index{synthetic-features!solving-problem-analysing-each-feature}

\textbf{Video}
\\
The video name / ID could be useful if a per-video classifier is also generated over and above the generic one. This together with other features could have some relevance.
\\
\\
\textbf{Comment ID}
\\
This feature does not have any relevance to the outcome whatsoever. It is a unique ID, built randomly, assigned to each comment. For this reason, it is out of scope for this discussion.
\\
\\
\textbf{Author and Date}
\\
As described earlier these two features independently do not have much of a direct impact on the outcome, however a synthetic feature could be generated which might have some form of effect on the outcome: A ratio of comment count over a time period for a particular author.
The idea is to make it easier for the algorithm to detect a potential pattern related to volume over a typical short period, thus the definition of time period can be assigned via testing.
\\
\\
\textbf{Comment}
\\
The comment body is not an easy feature and it could grow into a number of features, however it is the most relevant input for this spam classifier.
Quite a number of features could be exported from this comment, and most of them relate to natural language processing techniques. For this reason, output quality could also vary based on the language in context.
Some example features that could be extrapolated:

\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \resizebox{\textwidth}{!}{\begin{tabular}{lll}
      \toprule
                        & \textit{Synthetic Feature} & \textit{Scope / Description} \\
      \midrule
      \textit & \makecell[tl]{Language} & \makecell[tl]{This depends on the availabilities of various NLP implementations for \\ different languages, however one could have an indication of spam / \\ non-spam probabilities based on the comment languages for each \\ particular video.} \\
      \textit & \makecell[tl]{Readability \\ score} & \makecell[tl]{A readability score could be calculated per comment which gives an \\ indication on the quality of such text. An example of such a score \\ could be the \emph{Flesch Reading Ease} score.} \\
      \textit & \makecell[tl]{Length \\ (excl. stop words)} & \makecell[tl]{Very short or very long comments might have a probabilistic impact \\ on the outcome.} \\
      \textit  & \makecell[tl]{Presence of account \\ tags / URLs / emojis} & \makecell[tl]{The presence of account tags (ex. a Twitter username), URLs or emojis \\ could increase probability of the comment being spam.} \\
      \bottomrule
    \end{tabular}}
    \label{tab:sf_commentextractedfeatures}
\end{table}
\vspace{2mm}

\subsection{Updated feature / data set} \index{synthetic-features!updated-feature-dataset}

Following the synthetic feature generation described above, the updated data set used as an example here would look as follows:

\begin{table}[ht]
    \centering
    \fontfamily{ppl}\selectfont
    \resizebox{\textwidth}{!}{\begin{tabular}{llllllllll}
      \toprule
                        & \textit{\makecell{Video}} & \textit{\makecell{Author Comments \\ in last minute}} & \textit{\makecell{Language}} & \textit{\makecell{Readability}} & \textit{\makecell{Length excl. \\ stop words}} & \textit{\makecell{Presence of \\ account tags}} & \textit{\makecell{Presence of \\ URLs}} & \textit{\makecell{Presence of \\ emojis}} & \textit{\makecell{Class}} \\
      \midrule
      \textit & Psy & 1 & EN & 94.3 & 3 & No & Yes & No & Spam  \\
      \textit & Psy & 1 & EN & 103 & 3 & No & No & No & Ham  \\
      \textit & Psy & 1 & EN & 83.3 & 3 & No & No & No & Ham  \\
      \textit & Psy & 1 & EN & 32.6 & 3 & Yes & No & No & Spam  \\
      \bottomrule
    \end{tabular}}
    \caption{Updated sample of the four rows from the Psy dataset from the YouTube comment training dataset now containg the synthetic features.}
    \label{tab:sf_updatedds}
\end{table}
\vspace{2mm}

Looking at the output above, the effect of synthetic features can immediately be appreciated, as with such new features more meaning is given to the original dataset.

Naturally the above contains just a sample, and one must experiment with:

\begin{itemize}
    \item more or less synthetic features
    \item a further iteration of synthetic features from the generated features
    \item a much bigger data-set (the example above is too small to build a reliable classifier)
    \item perform feature selection (such as Principle Component Analysis) to identify the features that actually matter and remove extra noise
\end{itemize}

Therefore, employing a synthetic feature approach on your dataset as a pre-processing step, should in general give you positive results.